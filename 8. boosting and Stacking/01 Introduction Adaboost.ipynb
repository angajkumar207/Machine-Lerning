{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "803bae28-2e4d-48e5-8829-2ba6a1230816",
   "metadata": {},
   "source": [
    "***AdaBoost (Adaptive Boosting)***: A powerful ensemble machine learning algorithm that combines multiple simple\n",
    "\"weak\" models (like shallow decision trees called stumps) sequentially to create one strong, highly accurate model, \n",
    "focusing iteratively on correcting errors from previous models by increasing weights for misclassified data points. \n",
    "\n",
    "⭐ **Key Concept**\n",
    "- AdaBoost trains weak learners sequentially, and each new learner focuses more on the data points that previous  learners misclassified. It “boosts” the performance by adapting to mistakes.\n",
    "\n",
    "### How It Works (Step-by-Step):\n",
    "- 1. Start with equal weights for all training samples.\n",
    "  2. Train a weak model (e.g., a shallow tree).\n",
    "  3. Increase the weights of the misclassified samples so the next model pays more attention to them.\n",
    "  4. Train the next weak learner.\n",
    "  5. Combine all learners using a weighted majority vote (classification) or weighted sum (regression).\n",
    "\n",
    "## Why It Works Well\n",
    "- Reduces bias.\n",
    "- Often performs better than a single decision tree.\n",
    "- Simple and fast to train.\n",
    "- Works well even with weak models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
